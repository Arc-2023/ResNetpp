{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 8029435,
     "sourceType": "datasetVersion",
     "datasetId": 4732516
    }
   ],
   "dockerImageVersionId": 30674,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "RootDataset"
   ],
   "metadata": {},
   "id": "99642860b0463de3"
  },
  {
   "cell_type": "code",
   "source": [
    "# !pip install einops -U\n",
    "# !pip install -U ipywidgets==8.0.0\n",
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 -U\n",
    "# !pip install -U ray[train]==2.8.1"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-05T04:03:16.633588Z",
     "iopub.execute_input": "2024-04-05T04:03:16.634045Z",
     "iopub.status.idle": "2024-04-05T04:03:30.999569Z",
     "shell.execute_reply.started": "2024-04-05T04:03:16.634009Z",
     "shell.execute_reply": "2024-04-05T04:03:30.998596Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": [],
   "id": "939d607cefc854d0"
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "base_TBAD_csv_path = r\"D:/dataset/med/imageTBAD/dataframe.csv\""
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-05T04:03:31.001495Z",
     "iopub.execute_input": "2024-04-05T04:03:31.001801Z",
     "iopub.status.idle": "2024-04-05T04:03:35.485023Z",
     "shell.execute_reply.started": "2024-04-05T04:03:31.001774Z",
     "shell.execute_reply": "2024-04-05T04:03:35.484143Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-05T05:07:17.860028600Z",
     "start_time": "2024-04-05T05:07:15.043314700Z"
    }
   },
   "execution_count": 1,
   "outputs": [],
   "id": "166a731a23b30668"
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "class ResidualConv(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, stride, padding):\n",
    "        super(ResidualConv, self).__init__()\n",
    "\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.BatchNorm2d(input_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                input_dim, output_dim, kernel_size=3, stride=stride, padding=padding\n",
    "            ),\n",
    "            nn.BatchNorm2d(output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(output_dim, output_dim, kernel_size=3, padding=1),\n",
    "        )\n",
    "        self.conv_skip = nn.Sequential(\n",
    "            nn.Conv2d(input_dim, output_dim, kernel_size=3, stride=stride, padding=1),\n",
    "            nn.BatchNorm2d(output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_block(x) + self.conv_skip(x)\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, kernel, stride):\n",
    "        super(Upsample, self).__init__()\n",
    "\n",
    "        self.upsample = nn.ConvTranspose2d(\n",
    "            input_dim, output_dim, kernel_size=kernel, stride=stride\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.upsample(x)\n",
    "\n",
    "\n",
    "class Squeeze_Excite_Block(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(Squeeze_Excite_Block, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_dims, out_dims, rate=[6, 12, 18]):\n",
    "        super(ASPP, self).__init__()\n",
    "\n",
    "        self.aspp_block1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_dims, out_dims, 3, stride=1, padding=rate[0], dilation=rate[0]\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(out_dims),\n",
    "        )\n",
    "        self.aspp_block2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_dims, out_dims, 3, stride=1, padding=rate[1], dilation=rate[1]\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(out_dims),\n",
    "        )\n",
    "        self.aspp_block3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_dims, out_dims, 3, stride=1, padding=rate[2], dilation=rate[2]\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(out_dims),\n",
    "        )\n",
    "\n",
    "        self.output = nn.Conv2d(len(rate) * out_dims, out_dims, 1)\n",
    "        self._init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.aspp_block1(x)\n",
    "        x2 = self.aspp_block2(x)\n",
    "        x3 = self.aspp_block3(x)\n",
    "        out = torch.cat([x1, x2, x3], dim=1)\n",
    "        return self.output(out)\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "class Upsample_(nn.Module):\n",
    "    def __init__(self, scale=2):\n",
    "        super(Upsample_, self).__init__()\n",
    "\n",
    "        self.upsample = nn.Upsample(mode=\"bilinear\", scale_factor=scale)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.upsample(x)\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, input_encoder, input_decoder, output_dim):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "\n",
    "        self.conv_encoder = nn.Sequential(\n",
    "            nn.BatchNorm2d(input_encoder),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(input_encoder, output_dim, 3, padding=1),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "\n",
    "        self.conv_decoder = nn.Sequential(\n",
    "            nn.BatchNorm2d(input_decoder),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(input_decoder, output_dim, 3, padding=1),\n",
    "        )\n",
    "\n",
    "        self.conv_attn = nn.Sequential(\n",
    "            nn.BatchNorm2d(output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(output_dim, 1, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        out = self.conv_encoder(x1) + self.conv_decoder(x2)\n",
    "        out = self.conv_attn(out)\n",
    "        return out * x2\n",
    "\n",
    "\n",
    "class ResUnetPlusPlus(nn.Module):\n",
    "    def __init__(self, in_channel: int = 3, out_channel: int = 1, filters: list[int] = [32, 64, 128, 256, 512]):\n",
    "        super(ResUnetPlusPlus, self).__init__()\n",
    "\n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, filters[0], kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(filters[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(filters[0], filters[0], kernel_size=3, padding=1),\n",
    "        )\n",
    "        self.input_skip = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, filters[0], kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        self.squeeze_excite1 = Squeeze_Excite_Block(filters[0])\n",
    "\n",
    "        self.residual_conv1 = ResidualConv(filters[0], filters[1], 2, 1)\n",
    "\n",
    "        self.squeeze_excite2 = Squeeze_Excite_Block(filters[1])\n",
    "\n",
    "        self.residual_conv2 = ResidualConv(filters[1], filters[2], 2, 1)\n",
    "\n",
    "        self.squeeze_excite3 = Squeeze_Excite_Block(filters[2])\n",
    "\n",
    "        self.residual_conv3 = ResidualConv(filters[2], filters[3], 2, 1)\n",
    "\n",
    "        self.aspp_bridge = ASPP(filters[3], filters[4])\n",
    "\n",
    "        self.attn1 = AttentionBlock(filters[2], filters[4], filters[4])\n",
    "        self.upsample1 = Upsample_(2)\n",
    "        self.up_residual_conv1 = ResidualConv(filters[4] + filters[2], filters[3], 1, 1)\n",
    "\n",
    "        self.attn2 = AttentionBlock(filters[1], filters[3], filters[3])\n",
    "        self.upsample2 = Upsample_(2)\n",
    "        self.up_residual_conv2 = ResidualConv(filters[3] + filters[1], filters[2], 1, 1)\n",
    "\n",
    "        self.attn3 = AttentionBlock(filters[0], filters[2], filters[2])\n",
    "        self.upsample3 = Upsample_(2)\n",
    "        self.up_residual_conv3 = ResidualConv(filters[2] + filters[0], filters[1], 1, 1)\n",
    "\n",
    "        self.aspp_out = ASPP(filters[1], filters[0])\n",
    "\n",
    "        self.output_layer = nn.Sequential(nn.Conv2d(filters[0], out_channel, 1),\n",
    "                                          nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.input_layer(x) + self.input_skip(x)\n",
    "\n",
    "        x2 = self.squeeze_excite1(x1)\n",
    "        x2 = self.residual_conv1(x2)\n",
    "\n",
    "        x3 = self.squeeze_excite2(x2)\n",
    "        x3 = self.residual_conv2(x3)\n",
    "\n",
    "        x4 = self.squeeze_excite3(x3)\n",
    "        x4 = self.residual_conv3(x4)\n",
    "\n",
    "        x5 = self.aspp_bridge(x4)\n",
    "\n",
    "        x6 = self.attn1(x3, x5)\n",
    "        x6 = self.upsample1(x6)\n",
    "        x6 = torch.cat([x6, x3], dim=1)\n",
    "        x6 = self.up_residual_conv1(x6)\n",
    "\n",
    "        x7 = self.attn2(x2, x6)\n",
    "        x7 = self.upsample2(x7)\n",
    "        x7 = torch.cat([x7, x2], dim=1)\n",
    "        x7 = self.up_residual_conv2(x7)\n",
    "\n",
    "        x8 = self.attn3(x1, x7)\n",
    "        x8 = self.upsample3(x8)\n",
    "        x8 = torch.cat([x8, x1], dim=1)\n",
    "        x8 = self.up_residual_conv3(x8)\n",
    "\n",
    "        x9 = self.aspp_out(x8)\n",
    "        out = self.output_layer(x9)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "model = ResUnetPlusPlus(3).cuda()\n",
    "summary(model, input_size=(1, 3, 512, 512))\n"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2024-04-05T04:03:40.501306Z",
     "iopub.execute_input": "2024-04-05T04:03:40.501605Z",
     "iopub.status.idle": "2024-04-05T04:03:42.138048Z",
     "shell.execute_reply.started": "2024-04-05T04:03:40.501582Z",
     "shell.execute_reply": "2024-04-05T04:03:42.137183Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-05T05:07:20.019727700Z",
     "start_time": "2024-04-05T05:07:18.604400400Z"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nResUnetPlusPlus                          [1, 1, 512, 512]          --\n├─Sequential: 1-1                        [1, 32, 512, 512]         --\n│    └─Conv2d: 2-1                       [1, 32, 512, 512]         896\n│    └─BatchNorm2d: 2-2                  [1, 32, 512, 512]         64\n│    └─ReLU: 2-3                         [1, 32, 512, 512]         --\n│    └─Conv2d: 2-4                       [1, 32, 512, 512]         9,248\n├─Sequential: 1-2                        [1, 32, 512, 512]         --\n│    └─Conv2d: 2-5                       [1, 32, 512, 512]         896\n├─Squeeze_Excite_Block: 1-3              [1, 32, 512, 512]         --\n│    └─AdaptiveAvgPool2d: 2-6            [1, 32, 1, 1]             --\n│    └─Sequential: 2-7                   [1, 32]                   --\n│    │    └─Linear: 3-1                  [1, 2]                    64\n│    │    └─ReLU: 3-2                    [1, 2]                    --\n│    │    └─Linear: 3-3                  [1, 32]                   64\n│    │    └─Sigmoid: 3-4                 [1, 32]                   --\n├─ResidualConv: 1-4                      [1, 64, 256, 256]         --\n│    └─Sequential: 2-8                   [1, 64, 256, 256]         --\n│    │    └─BatchNorm2d: 3-5             [1, 32, 512, 512]         64\n│    │    └─ReLU: 3-6                    [1, 32, 512, 512]         --\n│    │    └─Conv2d: 3-7                  [1, 64, 256, 256]         18,496\n│    │    └─BatchNorm2d: 3-8             [1, 64, 256, 256]         128\n│    │    └─ReLU: 3-9                    [1, 64, 256, 256]         --\n│    │    └─Conv2d: 3-10                 [1, 64, 256, 256]         36,928\n│    └─Sequential: 2-9                   [1, 64, 256, 256]         --\n│    │    └─Conv2d: 3-11                 [1, 64, 256, 256]         18,496\n│    │    └─BatchNorm2d: 3-12            [1, 64, 256, 256]         128\n├─Squeeze_Excite_Block: 1-5              [1, 64, 256, 256]         --\n│    └─AdaptiveAvgPool2d: 2-10           [1, 64, 1, 1]             --\n│    └─Sequential: 2-11                  [1, 64]                   --\n│    │    └─Linear: 3-13                 [1, 4]                    256\n│    │    └─ReLU: 3-14                   [1, 4]                    --\n│    │    └─Linear: 3-15                 [1, 64]                   256\n│    │    └─Sigmoid: 3-16                [1, 64]                   --\n├─ResidualConv: 1-6                      [1, 128, 128, 128]        --\n│    └─Sequential: 2-12                  [1, 128, 128, 128]        --\n│    │    └─BatchNorm2d: 3-17            [1, 64, 256, 256]         128\n│    │    └─ReLU: 3-18                   [1, 64, 256, 256]         --\n│    │    └─Conv2d: 3-19                 [1, 128, 128, 128]        73,856\n│    │    └─BatchNorm2d: 3-20            [1, 128, 128, 128]        256\n│    │    └─ReLU: 3-21                   [1, 128, 128, 128]        --\n│    │    └─Conv2d: 3-22                 [1, 128, 128, 128]        147,584\n│    └─Sequential: 2-13                  [1, 128, 128, 128]        --\n│    │    └─Conv2d: 3-23                 [1, 128, 128, 128]        73,856\n│    │    └─BatchNorm2d: 3-24            [1, 128, 128, 128]        256\n├─Squeeze_Excite_Block: 1-7              [1, 128, 128, 128]        --\n│    └─AdaptiveAvgPool2d: 2-14           [1, 128, 1, 1]            --\n│    └─Sequential: 2-15                  [1, 128]                  --\n│    │    └─Linear: 3-25                 [1, 8]                    1,024\n│    │    └─ReLU: 3-26                   [1, 8]                    --\n│    │    └─Linear: 3-27                 [1, 128]                  1,024\n│    │    └─Sigmoid: 3-28                [1, 128]                  --\n├─ResidualConv: 1-8                      [1, 256, 64, 64]          --\n│    └─Sequential: 2-16                  [1, 256, 64, 64]          --\n│    │    └─BatchNorm2d: 3-29            [1, 128, 128, 128]        256\n│    │    └─ReLU: 3-30                   [1, 128, 128, 128]        --\n│    │    └─Conv2d: 3-31                 [1, 256, 64, 64]          295,168\n│    │    └─BatchNorm2d: 3-32            [1, 256, 64, 64]          512\n│    │    └─ReLU: 3-33                   [1, 256, 64, 64]          --\n│    │    └─Conv2d: 3-34                 [1, 256, 64, 64]          590,080\n│    └─Sequential: 2-17                  [1, 256, 64, 64]          --\n│    │    └─Conv2d: 3-35                 [1, 256, 64, 64]          295,168\n│    │    └─BatchNorm2d: 3-36            [1, 256, 64, 64]          512\n├─ASPP: 1-9                              [1, 512, 64, 64]          --\n│    └─Sequential: 2-18                  [1, 512, 64, 64]          --\n│    │    └─Conv2d: 3-37                 [1, 512, 64, 64]          1,180,160\n│    │    └─ReLU: 3-38                   [1, 512, 64, 64]          --\n│    │    └─BatchNorm2d: 3-39            [1, 512, 64, 64]          1,024\n│    └─Sequential: 2-19                  [1, 512, 64, 64]          --\n│    │    └─Conv2d: 3-40                 [1, 512, 64, 64]          1,180,160\n│    │    └─ReLU: 3-41                   [1, 512, 64, 64]          --\n│    │    └─BatchNorm2d: 3-42            [1, 512, 64, 64]          1,024\n│    └─Sequential: 2-20                  [1, 512, 64, 64]          --\n│    │    └─Conv2d: 3-43                 [1, 512, 64, 64]          1,180,160\n│    │    └─ReLU: 3-44                   [1, 512, 64, 64]          --\n│    │    └─BatchNorm2d: 3-45            [1, 512, 64, 64]          1,024\n│    └─Conv2d: 2-21                      [1, 512, 64, 64]          786,944\n├─AttentionBlock: 1-10                   [1, 512, 64, 64]          --\n│    └─Sequential: 2-22                  [1, 512, 64, 64]          --\n│    │    └─BatchNorm2d: 3-46            [1, 128, 128, 128]        256\n│    │    └─ReLU: 3-47                   [1, 128, 128, 128]        --\n│    │    └─Conv2d: 3-48                 [1, 512, 128, 128]        590,336\n│    │    └─MaxPool2d: 3-49              [1, 512, 64, 64]          --\n│    └─Sequential: 2-23                  [1, 512, 64, 64]          --\n│    │    └─BatchNorm2d: 3-50            [1, 512, 64, 64]          1,024\n│    │    └─ReLU: 3-51                   [1, 512, 64, 64]          --\n│    │    └─Conv2d: 3-52                 [1, 512, 64, 64]          2,359,808\n│    └─Sequential: 2-24                  [1, 1, 64, 64]            --\n│    │    └─BatchNorm2d: 3-53            [1, 512, 64, 64]          1,024\n│    │    └─ReLU: 3-54                   [1, 512, 64, 64]          --\n│    │    └─Conv2d: 3-55                 [1, 1, 64, 64]            513\n├─Upsample_: 1-11                        [1, 512, 128, 128]        --\n│    └─Upsample: 2-25                    [1, 512, 128, 128]        --\n├─ResidualConv: 1-12                     [1, 256, 128, 128]        --\n│    └─Sequential: 2-26                  [1, 256, 128, 128]        --\n│    │    └─BatchNorm2d: 3-56            [1, 640, 128, 128]        1,280\n│    │    └─ReLU: 3-57                   [1, 640, 128, 128]        --\n│    │    └─Conv2d: 3-58                 [1, 256, 128, 128]        1,474,816\n│    │    └─BatchNorm2d: 3-59            [1, 256, 128, 128]        512\n│    │    └─ReLU: 3-60                   [1, 256, 128, 128]        --\n│    │    └─Conv2d: 3-61                 [1, 256, 128, 128]        590,080\n│    └─Sequential: 2-27                  [1, 256, 128, 128]        --\n│    │    └─Conv2d: 3-62                 [1, 256, 128, 128]        1,474,816\n│    │    └─BatchNorm2d: 3-63            [1, 256, 128, 128]        512\n├─AttentionBlock: 1-13                   [1, 256, 128, 128]        --\n│    └─Sequential: 2-28                  [1, 256, 128, 128]        --\n│    │    └─BatchNorm2d: 3-64            [1, 64, 256, 256]         128\n│    │    └─ReLU: 3-65                   [1, 64, 256, 256]         --\n│    │    └─Conv2d: 3-66                 [1, 256, 256, 256]        147,712\n│    │    └─MaxPool2d: 3-67              [1, 256, 128, 128]        --\n│    └─Sequential: 2-29                  [1, 256, 128, 128]        --\n│    │    └─BatchNorm2d: 3-68            [1, 256, 128, 128]        512\n│    │    └─ReLU: 3-69                   [1, 256, 128, 128]        --\n│    │    └─Conv2d: 3-70                 [1, 256, 128, 128]        590,080\n│    └─Sequential: 2-30                  [1, 1, 128, 128]          --\n│    │    └─BatchNorm2d: 3-71            [1, 256, 128, 128]        512\n│    │    └─ReLU: 3-72                   [1, 256, 128, 128]        --\n│    │    └─Conv2d: 3-73                 [1, 1, 128, 128]          257\n├─Upsample_: 1-14                        [1, 256, 256, 256]        --\n│    └─Upsample: 2-31                    [1, 256, 256, 256]        --\n├─ResidualConv: 1-15                     [1, 128, 256, 256]        --\n│    └─Sequential: 2-32                  [1, 128, 256, 256]        --\n│    │    └─BatchNorm2d: 3-74            [1, 320, 256, 256]        640\n│    │    └─ReLU: 3-75                   [1, 320, 256, 256]        --\n│    │    └─Conv2d: 3-76                 [1, 128, 256, 256]        368,768\n│    │    └─BatchNorm2d: 3-77            [1, 128, 256, 256]        256\n│    │    └─ReLU: 3-78                   [1, 128, 256, 256]        --\n│    │    └─Conv2d: 3-79                 [1, 128, 256, 256]        147,584\n│    └─Sequential: 2-33                  [1, 128, 256, 256]        --\n│    │    └─Conv2d: 3-80                 [1, 128, 256, 256]        368,768\n│    │    └─BatchNorm2d: 3-81            [1, 128, 256, 256]        256\n├─AttentionBlock: 1-16                   [1, 128, 256, 256]        --\n│    └─Sequential: 2-34                  [1, 128, 256, 256]        --\n│    │    └─BatchNorm2d: 3-82            [1, 32, 512, 512]         64\n│    │    └─ReLU: 3-83                   [1, 32, 512, 512]         --\n│    │    └─Conv2d: 3-84                 [1, 128, 512, 512]        36,992\n│    │    └─MaxPool2d: 3-85              [1, 128, 256, 256]        --\n│    └─Sequential: 2-35                  [1, 128, 256, 256]        --\n│    │    └─BatchNorm2d: 3-86            [1, 128, 256, 256]        256\n│    │    └─ReLU: 3-87                   [1, 128, 256, 256]        --\n│    │    └─Conv2d: 3-88                 [1, 128, 256, 256]        147,584\n│    └─Sequential: 2-36                  [1, 1, 256, 256]          --\n│    │    └─BatchNorm2d: 3-89            [1, 128, 256, 256]        256\n│    │    └─ReLU: 3-90                   [1, 128, 256, 256]        --\n│    │    └─Conv2d: 3-91                 [1, 1, 256, 256]          129\n├─Upsample_: 1-17                        [1, 128, 512, 512]        --\n│    └─Upsample: 2-37                    [1, 128, 512, 512]        --\n├─ResidualConv: 1-18                     [1, 64, 512, 512]         --\n│    └─Sequential: 2-38                  [1, 64, 512, 512]         --\n│    │    └─BatchNorm2d: 3-92            [1, 160, 512, 512]        320\n│    │    └─ReLU: 3-93                   [1, 160, 512, 512]        --\n│    │    └─Conv2d: 3-94                 [1, 64, 512, 512]         92,224\n│    │    └─BatchNorm2d: 3-95            [1, 64, 512, 512]         128\n│    │    └─ReLU: 3-96                   [1, 64, 512, 512]         --\n│    │    └─Conv2d: 3-97                 [1, 64, 512, 512]         36,928\n│    └─Sequential: 2-39                  [1, 64, 512, 512]         --\n│    │    └─Conv2d: 3-98                 [1, 64, 512, 512]         92,224\n│    │    └─BatchNorm2d: 3-99            [1, 64, 512, 512]         128\n├─ASPP: 1-19                             [1, 32, 512, 512]         --\n│    └─Sequential: 2-40                  [1, 32, 512, 512]         --\n│    │    └─Conv2d: 3-100                [1, 32, 512, 512]         18,464\n│    │    └─ReLU: 3-101                  [1, 32, 512, 512]         --\n│    │    └─BatchNorm2d: 3-102           [1, 32, 512, 512]         64\n│    └─Sequential: 2-41                  [1, 32, 512, 512]         --\n│    │    └─Conv2d: 3-103                [1, 32, 512, 512]         18,464\n│    │    └─ReLU: 3-104                  [1, 32, 512, 512]         --\n│    │    └─BatchNorm2d: 3-105           [1, 32, 512, 512]         64\n│    └─Sequential: 2-42                  [1, 32, 512, 512]         --\n│    │    └─Conv2d: 3-106                [1, 32, 512, 512]         18,464\n│    │    └─ReLU: 3-107                  [1, 32, 512, 512]         --\n│    │    └─BatchNorm2d: 3-108           [1, 32, 512, 512]         64\n│    └─Conv2d: 2-43                      [1, 32, 512, 512]         3,104\n├─Sequential: 1-20                       [1, 1, 512, 512]          --\n│    └─Conv2d: 2-44                      [1, 1, 512, 512]          33\n│    └─Sigmoid: 2-45                     [1, 1, 512, 512]          --\n==========================================================================================\nTotal params: 14,482,564\nTrainable params: 14,482,564\nNon-trainable params: 0\nTotal mult-adds (Units.GIGABYTES): 282.58\n==========================================================================================\nInput size (MB): 3.15\nForward/backward pass size (MB): 3970.60\nParams size (MB): 57.93\nEstimated Total Size (MB): 4031.67\n=========================================================================================="
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "id": "a71d1a7a57059b5d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Config"
   ],
   "metadata": {},
   "id": "fc024892424dff6e"
  },
  {
   "cell_type": "code",
   "source": [
    "import os.path\n",
    "\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from rich.console import Console\n",
    "\n",
    "console = Console()\n",
    "\n",
    "\n",
    "class TBADaRoottaset(Dataset):\n",
    "    def __init__(self, csv_path: str = base_TBAD_csv_path):\n",
    "        self.df: pd.DataFrame = pd.read_csv(csv_path, delimiter=',')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        # console.log({\"image\": row['img'], 'mask': row['mask']})\n",
    "        img_path = os.path.join(row['img'])\n",
    "        mask_path = os.path.join(row['mask'])\n",
    "        return {\"image\": img_path, 'mask': mask_path}\n",
    "\n",
    "\n",
    "class DatasetofEachNII(Dataset):\n",
    "    def __init__(self, nii_image_path: str, nii_label_path: str):\n",
    "        console.print(f'[bold green]Loading {nii_image_path} and {nii_label_path}[/bold green]')\n",
    "        img = nib.load(''.join(nii_image_path))\n",
    "        self.img_data = img.get_fdata()\n",
    "\n",
    "        mask = nib.load(''.join(nii_label_path))\n",
    "        self.mask_data = mask.get_fdata()\n",
    "        if self.mask_data.ndim < self.img_data.ndim or self.mask_data.shape[2] < self.img_data.shape[2]:\n",
    "            # mask_data = np.zeros_like(img_data) + mask_data\n",
    "            self.mask_data = np.pad(self.mask_data,\n",
    "                                    (0, self.img_data.shape[2] - self.mask_data.shape[2]),\n",
    "                                    'constant',\n",
    "                                    constant_values=0)\n",
    "        self.trans_train_data = v2.Compose([\n",
    "            v2.ToImage(),\n",
    "            v2.ToDtype(torch.float16, scale=True),\n",
    "            v2.Normalize([1], [10.0]),\n",
    "            # v2.Lambda(lambda x: x / 255),\n",
    "            #             v2.Resize((280, 280)),\n",
    "            # v2.Resize((384, 384)),\n",
    "            v2.ConvertImageDtype(torch.float),\n",
    "        ])\n",
    "        self.trans_label = v2.Compose([\n",
    "            v2.ToImage(),\n",
    "            # v2.\n",
    "            # v2.Resize((384, 384)),\n",
    "            #             v2.Resize((280, 280)),\n",
    "            v2.ToDtype(torch.long)\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.img_data.shape[2]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = torch.as_tensor(self.img_data[:, :, idx])\n",
    "        mask_d = torch.as_tensor(self.mask_data[:, :, idx])\n",
    "        return {'image': self.trans_train_data(image), 'label': self.trans_label(mask_d)}\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-05T04:18:30.637441Z",
     "iopub.execute_input": "2024-04-05T04:18:30.638323Z",
     "iopub.status.idle": "2024-04-05T04:18:30.656068Z",
     "shell.execute_reply.started": "2024-04-05T04:18:30.638288Z",
     "shell.execute_reply": "2024-04-05T04:18:30.654955Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-05T05:07:24.893492800Z",
     "start_time": "2024-04-05T05:07:22.129854400Z"
    }
   },
   "execution_count": 3,
   "outputs": [],
   "id": "a1cae38b94c96fd3"
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from einops import reduce\n",
    "\n",
    "ALPHA = 0.8\n",
    "GAMMA = 2\n",
    "pp = 2\n",
    "smoo = 1\n",
    "epsilon = 1e-4\n",
    "\n",
    "\n",
    "class SoftDiceFocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "        SoftDiceFocalLoss类用于计算Soft Dice Focal损失。\n",
    "\n",
    "        参数:\n",
    "        weight: 一个可选的权重张量，用于在计算损失时对每个类别进行加权。\n",
    "        size_average: 一个布尔值，如果为True，则损失将通过平均所有元素来计算；如果为False，则损失将通过对所有元素求和来计算。\n",
    "\n",
    "        方法:\n",
    "        forward(inputs, targets, alpha=ALPHA, gamma=GAMMA): 计算Soft Dice Focal损失。\n",
    "        inputs: 输入张量，形状为[b, 4, h, w]。\n",
    "        targets: 目标张量，形状为[b, 1, h, w]，其中每个元素的值在0~3之间。\n",
    "        alpha: Focal损失的alpha参数，默认值为0.8。\n",
    "        gamma: Focal损失的gamma参数，默认值为2。\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SoftDiceFocalLoss, self).__init__()\n",
    "        self.p = pp\n",
    "        self.smooth = smoo\n",
    "        self.cross = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, inputs, targets, alpha=ALPHA, gamma=GAMMA):\n",
    "        # inputs: b 3 h w\n",
    "        # targets: b 1 h w   (0~3(int) in 1 dim)\n",
    "        # targets = targets.squeeze(1)\n",
    "        # print(\n",
    "        #     f'Inputs: {inputs.shape}; Targets: {targets.shape};targets max: {torch.max(targets)}; targets min: {torch.min(targets)}'))\n",
    "        targets = torch.squeeze(targets, 1)\n",
    "        targets = torch.where(targets.eq(3), 0, targets)\n",
    "\n",
    "        # fig, axes = plt.subplots(1, 2)\n",
    "        # axes[0].imshow(torch.where((targets[0].cpu() == 1), 1, 0), cmap='gray')\n",
    "        # axes[1].imshow(torch.where((targets[0].cpu() == 2), 1, 0), cmap='gray')\n",
    "\n",
    "        plt.show()\n",
    "        # print(f'Targets: {targets.shape}; Inputs: {inputs.shape}')\n",
    "        CE = self.cross(inputs, targets)\n",
    "        CE_EXP = torch.exp(-CE)\n",
    "        focal_loss = alpha * (1 - CE_EXP) ** gamma * CE\n",
    "        # # flatten label and prediction tensors\n",
    "        # inputs = (inputs[:, 1, :, :] + inputs[:, 2, :, :] + inputs[:, 3, :, :]).reshape(-1)\n",
    "\n",
    "        # targets: torch.Tensor = torch.unsqueeze(targets, 1)\n",
    "        # Iterate over each unique value\n",
    "\n",
    "        target_dice = torch.stack([torch.where(targets.eq(value), 1, 0) for value in range(3)], dim=1)\n",
    "\n",
    "        # dice_loss = self.Dice(inputs, target_dice)\n",
    "        dice_loss = 0\n",
    "        for i in range(inputs.shape[1]):  # iterate over each channel\n",
    "            input_channel = inputs[:, i, :, :]\n",
    "            target_channel = target_dice[:, i, :, :]\n",
    "            dice_loss = self.Dice(input_channel, target_channel)\n",
    "\n",
    "        total_loss = dice_loss + focal_loss\n",
    "\n",
    "        # + dice_loss\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def GenerilizeDiceLoss(self, dice_loss, input_channel, target_channel):\n",
    "        wei = torch.sum(target_channel, axis=[0, 2, 3])  # (n_class,)\n",
    "        wei = 1 / (wei ** 2 + epsilon)\n",
    "        intersection = torch.sum(wei * torch.sum(input_channel * target_channel, axis=[0, 2, 3]))\n",
    "        union = torch.sum(wei * torch.sum(input_channel + target_channel, axis=[0, 2, 3]))\n",
    "        dice_loss += 1 - (2. * intersection) / (union + epsilon)\n",
    "        return dice_loss\n",
    "\n",
    "    def Dice(self, inputs, target_dice):\n",
    "        probs = torch.sigmoid(inputs)\n",
    "        target = torch.sigmoid(target_dice)\n",
    "        # print(f'Probs: {probs.shape}; Target: {target.shape}')\n",
    "        # b 3 256 256 - b 256 256\n",
    "        probs = torch.reshape(probs, (-1,))\n",
    "        target = torch.reshape(target, (-1,))\n",
    "        # print(f'Probs: {probs.shape}; Target: {target.shape}')\n",
    "        numer = (probs * target).sum()\n",
    "        denor = (probs.pow(self.p) + target.pow(self.p)).sum()\n",
    "        dice_loss = 1. - (2 * numer + self.smooth) / (denor + self.smooth)\n",
    "        return dice_loss\n"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-05T05:07:25.791274600Z",
     "start_time": "2024-04-05T05:07:25.777197900Z"
    }
   },
   "execution_count": 4,
   "outputs": [],
   "id": "63c045cc077a6170"
  },
  {
   "cell_type": "markdown",
   "source": [
    "DDP"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d75327fb964d9816"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ProcessExitedException",
     "evalue": "process 0 terminated with exit code 1",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mProcessExitedException\u001B[0m                    Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[21], line 181\u001B[0m\n\u001B[0;32m    174\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_demo\u001B[39m(world_size):\n\u001B[0;32m    175\u001B[0m     mp\u001B[38;5;241m.\u001B[39mspawn(train_func,\n\u001B[0;32m    176\u001B[0m              args\u001B[38;5;241m=\u001B[39m(world_size,),\n\u001B[0;32m    177\u001B[0m              nprocs\u001B[38;5;241m=\u001B[39mworld_size,\n\u001B[0;32m    178\u001B[0m              join\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m--> 181\u001B[0m \u001B[43mrun_demo\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[21], line 175\u001B[0m, in \u001B[0;36mrun_demo\u001B[1;34m(world_size)\u001B[0m\n\u001B[0;32m    174\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_demo\u001B[39m(world_size):\n\u001B[1;32m--> 175\u001B[0m     \u001B[43mmp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mspawn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_func\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    176\u001B[0m \u001B[43m             \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mworld_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    177\u001B[0m \u001B[43m             \u001B[49m\u001B[43mnprocs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mworld_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    178\u001B[0m \u001B[43m             \u001B[49m\u001B[43mjoin\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\JETB\\AI\\venv\\Lib\\site-packages\\torch\\multiprocessing\\spawn.py:241\u001B[0m, in \u001B[0;36mspawn\u001B[1;34m(fn, args, nprocs, join, daemon, start_method)\u001B[0m\n\u001B[0;32m    235\u001B[0m     msg \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    236\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis method only supports start_method=spawn (got: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m).\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    237\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTo use a different start_method use:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    238\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m torch.multiprocessing.start_processes(...)\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m start_method\n\u001B[0;32m    239\u001B[0m     )\n\u001B[0;32m    240\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(msg)\n\u001B[1;32m--> 241\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mstart_processes\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnprocs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mjoin\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdaemon\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart_method\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mspawn\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\JETB\\AI\\venv\\Lib\\site-packages\\torch\\multiprocessing\\spawn.py:197\u001B[0m, in \u001B[0;36mstart_processes\u001B[1;34m(fn, args, nprocs, join, daemon, start_method)\u001B[0m\n\u001B[0;32m    194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m context\n\u001B[0;32m    196\u001B[0m \u001B[38;5;66;03m# Loop on join until it returns True or raises an exception.\u001B[39;00m\n\u001B[1;32m--> 197\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[43mcontext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m    198\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\JETB\\AI\\venv\\Lib\\site-packages\\torch\\multiprocessing\\spawn.py:148\u001B[0m, in \u001B[0;36mProcessContext.join\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    140\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m ProcessExitedException(\n\u001B[0;32m    141\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprocess \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m terminated with signal \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (error_index, name),\n\u001B[0;32m    142\u001B[0m             error_index\u001B[38;5;241m=\u001B[39merror_index,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    145\u001B[0m             signal_name\u001B[38;5;241m=\u001B[39mname,\n\u001B[0;32m    146\u001B[0m         )\n\u001B[0;32m    147\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 148\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m ProcessExitedException(\n\u001B[0;32m    149\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprocess \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m terminated with exit code \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (error_index, exitcode),\n\u001B[0;32m    150\u001B[0m             error_index\u001B[38;5;241m=\u001B[39merror_index,\n\u001B[0;32m    151\u001B[0m             error_pid\u001B[38;5;241m=\u001B[39mfailed_process\u001B[38;5;241m.\u001B[39mpid,\n\u001B[0;32m    152\u001B[0m             exit_code\u001B[38;5;241m=\u001B[39mexitcode,\n\u001B[0;32m    153\u001B[0m         )\n\u001B[0;32m    155\u001B[0m original_trace \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39merror_queues[error_index]\u001B[38;5;241m.\u001B[39mget()\n\u001B[0;32m    156\u001B[0m msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m-- Process \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m terminated with the following error:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m error_index\n",
      "\u001B[1;31mProcessExitedException\u001B[0m: process 0 terminated with exit code 1"
     ]
    }
   ],
   "source": [
    "\n",
    "import traceback\n",
    "import torch\n",
    "from torchvision.transforms import v2 as T\n",
    "\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "params = {\n",
    "    'epochs': 10,\n",
    "    'batch_size': 1,\n",
    "    'amp': True,\n",
    "    'shuffle': True,\n",
    "    'in_channel': 1,\n",
    "    'out_channel': 3,\n",
    "    'T_0': 1,\n",
    "    'T_mult': 2,\n",
    "    'proportion': 0.9,\n",
    "    'cos': True,\n",
    "}\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "transformation = T.Compose([\n",
    "    T.ToDtype(torch.float32),\n",
    "    T.ToPILImage(),\n",
    "])\n",
    "torch.manual_seed(3407)\n",
    "\n",
    "\n",
    "def train_func(rank, *args):\n",
    "    try:\n",
    "        # model = ResUnetPlusPlus(in_channel=params['in_channel'], out_channel=params['out_channel'])\n",
    "        UnetModel = ResUnetPlusPlus(in_channel=params['in_channel'], out_channel=params['out_channel']).to(rank)\n",
    "        UnetModel = DDP(UnetModel, device_ids=[rank])\n",
    "        setup(rank, args[0])\n",
    "        # model = ray.train.torch.prepare_model(model)\n",
    "        # loss_fn = SoftDiceLoss()\n",
    "        loss_fn = SoftDiceFocalLoss()\n",
    "        # loss_fn = nn.MSELoss()\n",
    "        opti = torch.optim.RAdam(UnetModel.parameters(), lr=0.003)\n",
    "        schd_lr = None\n",
    "        if params['cos']:\n",
    "            schd_lr = CosineAnnealingWarmRestarts(opti, T_0=params['T_0'], T_mult=params['T_mult'])\n",
    "\n",
    "        #     fig, axs = plt.subplots(2, 3)\n",
    "        scaler = torch.cuda.amp.GradScaler(init_scale=8192)\n",
    "        for epoch in tqdm(range(params['epochs']), leave=False):\n",
    "            rootset = TBADaRoottaset()\n",
    "            rootloader = DataLoader(rootset, batch_size=1, shuffle=params['shuffle'], drop_last=True)\n",
    "\n",
    "            for data in tqdm(rootloader, leave=False):\n",
    "                nii_set = DatasetofEachNII(nii_image_path=data['image'],\n",
    "                                           nii_label_path=data['mask'])\n",
    "                train_set, test_set = torch.utils.data.random_split(nii_set, [int(len(nii_set) * params['proportion']),\n",
    "                                                                              len(nii_set) - int(\n",
    "                                                                                  len(nii_set) * params['proportion'])])\n",
    "                ddp_train_sam = DistributedSampler(train_set)\n",
    "                ddp_test_sam = DistributedSampler(test_set)\n",
    "\n",
    "                train_loader = DataLoader(train_set, batch_size=params['batch_size'],\n",
    "                                          shuffle=True, drop_last=True,\n",
    "                                          collate_fn=col_fn, sampler=ddp_train_sam)\n",
    "                test_loader = DataLoader(test_set, batch_size=params['batch_size'],\n",
    "                                         shuffle=True, drop_last=True,\n",
    "                                         collate_fn=col_fn, sampler=ddp_test_sam)\n",
    "                # ray_train_loader = ray.train.torch.prepare_data_loader(train_dataloader)\n",
    "                # ray_test_loader = ray.train.torch.prepare_data_loader(test_dataloader)\n",
    "                train_loss = 0\n",
    "                for niidata in tqdm(train_loader, leave=False):\n",
    "                    img = niidata['images'].to(rank)\n",
    "                    label = niidata['labels'].to(rank)\n",
    "                    if params['amp']:\n",
    "                        with torch.autocast(device_type=rank):\n",
    "                            opti.zero_grad()\n",
    "                            UnetModel.train()\n",
    "                            pred = UnetModel(img)\n",
    "                            display_dynamicly(img, label, pred)\n",
    "                            loss_now = loss_fn(pred, label)\n",
    "                        train_loss += loss_now.item()\n",
    "                        scaler.scale(loss_now).backward()\n",
    "                        scaler.step(opti)\n",
    "                        if params['cos']:\n",
    "                            schd_lr.step()\n",
    "                        scaler.update()\n",
    "                    else:\n",
    "                        opti.zero_grad()\n",
    "                        UnetModel.train()\n",
    "                        pred = UnetModel(img)\n",
    "                        display_dynamicly(img, label, pred)\n",
    "                        loss_now = loss_fn(pred, label)\n",
    "                        train_loss += loss_now.item()\n",
    "                        loss_now.backward()\n",
    "                        opti.step()\n",
    "                        if params['cos']:\n",
    "                            schd_lr.step()\n",
    "                    print(f'Epoch: {epoch} Loss: {loss_now * params[\"batch_size\"]}')\n",
    "\n",
    "                test_loss = 0\n",
    "                for niidatatest in tqdm(test_loader, leave=False):\n",
    "                    with torch.no_grad():\n",
    "                        img = niidatatest['images'].to(rank)\n",
    "                        label = niidatatest['labels'].to(rank)\n",
    "                        UnetModel.eval()\n",
    "                        pred = UnetModel(img)\n",
    "                        loss = loss_fn(pred, label)\n",
    "                        test_loss += loss.item()\n",
    "                train_loss /= (len(train_loader) / params['batch_size'])\n",
    "                test_loss /= (len(test_loader) / params['batch_size'])\n",
    "                metrics = {\"train_loss\": train_loss, 'test_loss': test_loss, \"epoch\": epoch}\n",
    "                #             with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "                #                 torch.save(\n",
    "                #                     model.state_dict(),\n",
    "                #                     os.path.join(temp_checkpoint_dir, \"model.pt\")\n",
    "                #                 )\n",
    "                #                 ray.train.report(\n",
    "                #                     metrics,\n",
    "                #                     checkpoint=ray.train.Checkpoint.from_directory(temp_checkpoint_dir),\n",
    "                #                 )\n",
    "                #             if ray.train.get_context().get_world_rank() == 0:\n",
    "                print(metrics)\n",
    "                # del test_dataloader\n",
    "                # del train_dataloader\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in process {rank}: {e}\")\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        cleanup()\n",
    "\n",
    "\n",
    "def display_dynamicly(img, label, pred):\n",
    "    fig, axs = plt.subplots(2, 3)\n",
    "    random_index = random.randint(0, pred.shape[0] - 1)\n",
    "    axs[0, 0].imshow(transformation(pred[random_index][0]), cmap='gray')\n",
    "    axs[0, 0].title.set_text('Prediction: 0')\n",
    "    axs[0, 1].imshow(transformation(pred[random_index][1]), cmap='gray')\n",
    "    axs[0, 1].title.set_text('Prediction: 1')\n",
    "    axs[0, 2].imshow(transformation(pred[random_index][2]), cmap='gray')\n",
    "    axs[0, 2].title.set_text('Prediction: 2')\n",
    "    axs[1, 0].imshow(transformation(label[random_index]), cmap='gray')\n",
    "    axs[1, 0].title.set_text('Label')\n",
    "    axs[1, 1].imshow(transformation(img[random_index]), cmap='gray')\n",
    "    axs[1, 1].title.set_text('Image')\n",
    "    plt.show()\n",
    "\n",
    "    # plt.colorbar()\n",
    "    # Pause for a short period, allowing the plot to update\n",
    "    plt.pause(0.1)\n",
    "    #     # Clear the current axes\n",
    "    axs[0, 0].cla()\n",
    "    axs[0, 1].cla()\n",
    "    axs[0, 2].cla()\n",
    "\n",
    "    axs[1, 0].cla()\n",
    "    axs[1, 1].cla()\n",
    "    axs[1, 2].cla()\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "def col_fn(images):\n",
    "    # {\"image\": png_img, \"label\": png_label}\n",
    "    # image: 1*512* 512\n",
    "    inputs: torch.Tensor = torch.stack([image['image'] for image in images])\n",
    "    labels: torch.Tensor = torch.stack([image['label'] for image in images])\n",
    "    return {\"images\": inputs, \"labels\": labels}\n",
    "\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "\n",
    "    # initialize the process group\n",
    "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "\n",
    "def run_demo(world_size):\n",
    "    mp.spawn(train_func,\n",
    "             args=(world_size,),\n",
    "             nprocs=world_size,\n",
    "             join=True)\n",
    "\n",
    "\n",
    "run_demo(1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T06:34:03.885037800Z",
     "start_time": "2024-04-05T06:34:01.261130400Z"
    }
   },
   "id": "4cfbdce0056c8f9f",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.status.busy": "2024-04-05T04:03:44.691682Z",
     "iopub.status.idle": "2024-04-05T04:03:44.692015Z",
     "shell.execute_reply.started": "2024-04-05T04:03:44.691848Z",
     "shell.execute_reply": "2024-04-05T04:03:44.691862Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-04-05T06:33:04.832584800Z",
     "start_time": "2024-04-05T06:33:01.527806500Z"
    }
   },
   "execution_count": 20,
   "outputs": [
    {
     "ename": "ProcessExitedException",
     "evalue": "process 0 terminated with exit code 1",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mProcessExitedException\u001B[0m                    Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 18\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_demo\u001B[39m(world_size):\n\u001B[0;32m     12\u001B[0m     mp\u001B[38;5;241m.\u001B[39mspawn(train_func,\n\u001B[0;32m     13\u001B[0m              args\u001B[38;5;241m=\u001B[39m(world_size,),\n\u001B[0;32m     14\u001B[0m              nprocs\u001B[38;5;241m=\u001B[39mworld_size,\n\u001B[0;32m     15\u001B[0m              join\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m---> 18\u001B[0m \u001B[43mrun_demo\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[20], line 12\u001B[0m, in \u001B[0;36mrun_demo\u001B[1;34m(world_size)\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_demo\u001B[39m(world_size):\n\u001B[1;32m---> 12\u001B[0m     \u001B[43mmp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mspawn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_func\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[43m             \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mworld_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[43m             \u001B[49m\u001B[43mnprocs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mworld_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[43m             \u001B[49m\u001B[43mjoin\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\JETB\\AI\\venv\\Lib\\site-packages\\torch\\multiprocessing\\spawn.py:241\u001B[0m, in \u001B[0;36mspawn\u001B[1;34m(fn, args, nprocs, join, daemon, start_method)\u001B[0m\n\u001B[0;32m    235\u001B[0m     msg \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    236\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis method only supports start_method=spawn (got: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m).\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    237\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTo use a different start_method use:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    238\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m torch.multiprocessing.start_processes(...)\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m start_method\n\u001B[0;32m    239\u001B[0m     )\n\u001B[0;32m    240\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(msg)\n\u001B[1;32m--> 241\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mstart_processes\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnprocs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mjoin\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdaemon\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart_method\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mspawn\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\JETB\\AI\\venv\\Lib\\site-packages\\torch\\multiprocessing\\spawn.py:197\u001B[0m, in \u001B[0;36mstart_processes\u001B[1;34m(fn, args, nprocs, join, daemon, start_method)\u001B[0m\n\u001B[0;32m    194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m context\n\u001B[0;32m    196\u001B[0m \u001B[38;5;66;03m# Loop on join until it returns True or raises an exception.\u001B[39;00m\n\u001B[1;32m--> 197\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[43mcontext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m    198\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\JETB\\AI\\venv\\Lib\\site-packages\\torch\\multiprocessing\\spawn.py:148\u001B[0m, in \u001B[0;36mProcessContext.join\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    140\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m ProcessExitedException(\n\u001B[0;32m    141\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprocess \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m terminated with signal \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (error_index, name),\n\u001B[0;32m    142\u001B[0m             error_index\u001B[38;5;241m=\u001B[39merror_index,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    145\u001B[0m             signal_name\u001B[38;5;241m=\u001B[39mname,\n\u001B[0;32m    146\u001B[0m         )\n\u001B[0;32m    147\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 148\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m ProcessExitedException(\n\u001B[0;32m    149\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprocess \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m terminated with exit code \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (error_index, exitcode),\n\u001B[0;32m    150\u001B[0m             error_index\u001B[38;5;241m=\u001B[39merror_index,\n\u001B[0;32m    151\u001B[0m             error_pid\u001B[38;5;241m=\u001B[39mfailed_process\u001B[38;5;241m.\u001B[39mpid,\n\u001B[0;32m    152\u001B[0m             exit_code\u001B[38;5;241m=\u001B[39mexitcode,\n\u001B[0;32m    153\u001B[0m         )\n\u001B[0;32m    155\u001B[0m original_trace \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39merror_queues[error_index]\u001B[38;5;241m.\u001B[39mget()\n\u001B[0;32m    156\u001B[0m msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m-- Process \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m terminated with the following error:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m error_index\n",
      "\u001B[1;31mProcessExitedException\u001B[0m: process 0 terminated with exit code 1"
     ]
    }
   ],
   "id": "eb9eb45db15e6071"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from typing import Callable, Union\n",
    "\n",
    "import dill\n",
    "import ray\n",
    "import torch\n",
    "from PIL._imaging import draw\n",
    "from einops import rearrange\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from rich.console import Console\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.transforms import v2 as T\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.train import ScalingConfig\n",
    "import cloudpickle\n",
    "import random\n",
    "\n",
    "params = {\n",
    "    'epochs': 10,\n",
    "    'batch_size': 1,\n",
    "    'amp': True,\n",
    "    'shuffle': True,\n",
    "    'in_channel': 1,\n",
    "    'out_channel': 3,\n",
    "    'T_0': 1,\n",
    "    'T_mult': 2,\n",
    "    'proportion': 0.9,\n",
    "    'cos': True,\n",
    "}\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "transformation = T.Compose([\n",
    "    T.ToDtype(torch.float32),\n",
    "    T.ToPILImage(),\n",
    "])\n",
    "torch.manual_seed(3407)\n",
    "\n",
    "\n",
    "def train_func(rank, world_size):\n",
    "    # model = ResUnetPlusPlus(in_channel=params['in_channel'], out_channel=params['out_channel'])\n",
    "    model = ResUnetPlusPlus(in_channel=params['in_channel'], out_channel=params['out_channel']).to(rank)\n",
    "    # model = DDP(model, device_ids=[rank])\n",
    "    # setup(rank, world_size)\n",
    "    # model = ray.train.torch.prepare_model(model)\n",
    "    # loss_fn = SoftDiceLoss()\n",
    "    loss_fn = SoftDiceFocalLoss()\n",
    "    # loss_fn = nn.MSELoss()\n",
    "    opti = torch.optim.RAdam(model.parameters(), lr=0.003)\n",
    "    schd_lr = None\n",
    "    if params['cos']:\n",
    "        schd_lr = CosineAnnealingWarmRestarts(opti, T_0=params['T_0'], T_mult=params['T_mult'])\n",
    "\n",
    "    #     fig, axs = plt.subplots(2, 3)\n",
    "    scaler = torch.cuda.amp.GradScaler(init_scale=8192)\n",
    "    for epoch in tqdm(range(params['epochs']), leave=False):\n",
    "        rootset = TBADaRoottaset()\n",
    "        rootloader = DataLoader(rootset, batch_size=1, shuffle=params['shuffle'], drop_last=True)\n",
    "\n",
    "        for data in tqdm(rootloader, leave=False):\n",
    "            nii_set = DatasetofEachNII(nii_image_path=data['image'],\n",
    "                                       nii_label_path=data['mask'])\n",
    "            train_set, test_set = torch.utils.data.random_split(nii_set, [int(len(nii_set) * params['proportion']),\n",
    "                                                                          len(nii_set) - int(\n",
    "                                                                              len(nii_set) * params['proportion'])])\n",
    "\n",
    "            train_loader = DataLoader(train_set, batch_size=params['batch_size'],\n",
    "                                      shuffle=True, drop_last=True,\n",
    "                                      collate_fn=col_fn)\n",
    "            test_loader = DataLoader(test_set, batch_size=params['batch_size'],\n",
    "                                     shuffle=True, drop_last=True,\n",
    "                                     collate_fn=col_fn)\n",
    "            # ray_train_loader = ray.train.torch.prepare_data_loader(train_dataloader)\n",
    "            # ray_test_loader = ray.train.torch.prepare_data_loader(test_dataloader)\n",
    "            train_loss = 0\n",
    "            for niidata in tqdm(train_loader, leave=False):\n",
    "                img = niidata['images'].to(rank)\n",
    "                label = niidata['labels'].to(device)\n",
    "                if params['amp']:\n",
    "                    with torch.autocast(device_type=device):\n",
    "                        opti.zero_grad()\n",
    "                        model.train()\n",
    "                        pred = model(img)\n",
    "                        display_dynamicly(img, label, pred)\n",
    "                        loss_now = loss_fn(pred, label)\n",
    "                    train_loss += loss_now.item()\n",
    "                    scaler.scale(loss_now).backward()\n",
    "                    scaler.step(opti)\n",
    "                    if params['cos']:\n",
    "                        schd_lr.step()\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    opti.zero_grad()\n",
    "                    model.train()\n",
    "                    pred = model(img)\n",
    "                    display_dynamicly(img, label, pred)\n",
    "                    loss_now = loss_fn(pred, label)\n",
    "                    train_loss += loss_now.item()\n",
    "                    loss_now.backward()\n",
    "                    opti.step()\n",
    "                    if params['cos']:\n",
    "                        schd_lr.step()\n",
    "                print(f'Epoch: {epoch} Loss: {loss_now * params[\"batch_size\"]}')\n",
    "\n",
    "            test_loss = 0\n",
    "            for niidatatest in tqdm(test_loader, leave=False):\n",
    "                with torch.no_grad():\n",
    "                    img = niidatatest['images'].to(device)\n",
    "                    label = niidatatest['labels'].to(device)\n",
    "                    model.eval()\n",
    "                    pred = model(img)\n",
    "                    loss = loss_fn(pred, label)\n",
    "                    test_loss += loss.item()\n",
    "            train_loss /= (len(train_loader) / params['batch_size'])\n",
    "            test_loss /= (len(test_loader) / params['batch_size'])\n",
    "            metrics = {\"train_loss\": train_loss, 'test_loss': test_loss, \"epoch\": epoch}\n",
    "            #             with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "            #                 torch.save(\n",
    "            #                     model.state_dict(),\n",
    "            #                     os.path.join(temp_checkpoint_dir, \"model.pt\")\n",
    "            #                 )\n",
    "            #                 ray.train.report(\n",
    "            #                     metrics,\n",
    "            #                     checkpoint=ray.train.Checkpoint.from_directory(temp_checkpoint_dir),\n",
    "            #                 )\n",
    "            #             if ray.train.get_context().get_world_rank() == 0:\n",
    "            print(metrics)\n",
    "            # del test_dataloader\n",
    "            # del train_dataloader\n",
    "\n",
    "\n",
    "def display_dynamicly(img, label, pred):\n",
    "    fig, axs = plt.subplots(2, 3)\n",
    "    random_index = random.randint(0, pred.shape[0] - 1)\n",
    "    axs[0, 0].imshow(transformation(pred[random_index][0]), cmap='gray')\n",
    "    axs[0, 0].title.set_text('Prediction: 0')\n",
    "    axs[0, 1].imshow(transformation(pred[random_index][1]), cmap='gray')\n",
    "    axs[0, 1].title.set_text('Prediction: 1')\n",
    "    axs[0, 2].imshow(transformation(pred[random_index][2]), cmap='gray')\n",
    "    axs[0, 2].title.set_text('Prediction: 2')\n",
    "    axs[1, 0].imshow(transformation(label[random_index]), cmap='gray')\n",
    "    axs[1, 0].title.set_text('Label')\n",
    "    axs[1, 1].imshow(transformation(img[random_index]), cmap='gray')\n",
    "    axs[1, 1].title.set_text('Image')\n",
    "    plt.show()\n",
    "\n",
    "    # plt.colorbar()\n",
    "    # Pause for a short period, allowing the plot to update\n",
    "    plt.pause(0.1)\n",
    "    #     # Clear the current axes\n",
    "    axs[0, 0].cla()\n",
    "    axs[0, 1].cla()\n",
    "    axs[0, 2].cla()\n",
    "\n",
    "    axs[1, 0].cla()\n",
    "    axs[1, 1].cla()\n",
    "    axs[1, 2].cla()\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "def get_root_dataloader(param: dict):\n",
    "    rootset = TBADaRoottaset()\n",
    "    rootloader = DataLoader(rootset, batch_size=1, shuffle=param['shuffle'], drop_last=True)\n",
    "    return rootloader\n",
    "\n",
    "\n",
    "def get_nii_dataloader(param: dict, fn, nii_image_path: str, nii_label_path: str):\n",
    "    # console.print(f'[bold green]Loading {nii_image_path} and {nii_label_path}[/bold green]')\n",
    "    nii_set = DatasetofEachNII(nii_image_path, nii_label_path)\n",
    "    train_set, test_set = torch.utils.data.random_split(nii_set, [int(len(nii_set) * param['proportion']),\n",
    "                                                                  len(nii_set) - int(\n",
    "                                                                      len(nii_set) * param['proportion'])])\n",
    "    train_loader = DataLoader(train_set, batch_size=param['batch_size'], shuffle=param['shuffle'], drop_last=True,\n",
    "                              collate_fn=fn)\n",
    "    test_loader = DataLoader(test_set, batch_size=param['batch_size'], shuffle=param['shuffle'], drop_last=True,\n",
    "                             collate_fn=fn)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def getdataloader(param: dict, fn: Callable, root: bool, nii_image_path: str = None, nii_label_path: str = None) -> \\\n",
    "        Union[DataLoader,\n",
    "        tuple[DataLoader, DataLoader]]:\n",
    "    if root:\n",
    "        return get_root_dataloader(param)\n",
    "    else:\n",
    "        return get_nii_dataloader(param, fn, nii_image_path, nii_label_path)\n",
    "\n",
    "\n",
    "def col_fn(images):\n",
    "    # {\"image\": png_img, \"label\": png_label}\n",
    "    # image: 1*512* 512\n",
    "    inputs: torch.Tensor = torch.stack([image['image'] for image in images])\n",
    "    labels: torch.Tensor = torch.stack([image['label'] for image in images])\n",
    "    return {\"images\": inputs, \"labels\": labels}\n",
    "\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "\n",
    "    # initialize the process group\n",
    "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     ray.shutdown()\n",
    "#     ray.init(include_dashboard=False, dashboard_host='0.0.0.0', ignore_reinit_error=True, dashboard_port=8265)\n",
    "\n",
    "#     scaling_config = ScalingConfig(num_workers=1, use_gpu=True, resources_per_worker={\n",
    "#         \"CPU\": 4,\n",
    "#         \"GPU\": 2,\n",
    "#     }, )\n",
    "#     torch_config = ray.train.torch.TorchConfig(backend='gloo')\n",
    "#     trainer = ray.train.torch.TorchTrainer(train_func, train_loop_config=args, scaling_config=scaling_config,\n",
    "#                                            torch_config=torch_config)\n",
    "#     result = trainer.fit()\n",
    "# train_func()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-05T06:33:04.833841900Z"
    }
   },
   "id": "283dcdeed30975f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "655d9cfbf1174f95"
  }
 ]
}
